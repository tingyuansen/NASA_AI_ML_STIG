{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Large Language Models as Research Agents: Part 1 - API Basics\n",
        "\n",
        "*NASA Cosmic Origins AI/ML STIG Tutorial Series*\n",
        "\n",
        "*Lecture 1 of 2: Working with LLM APIs*\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand what APIs are and how to interact with LLMs programmatically\n",
        "- Make your first API calls to Claude and other language models\n",
        "- Master key parameters: temperature, max_tokens, system prompts\n",
        "- Build multi-turn conversations and manage context\n",
        "- Implement prompting strategies for research tasks\n",
        "- Work with vision models for astronomical image analysis\n",
        "- Handle rate limits and errors in production code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Large Language Models (LLMs) are rapidly transforming how we conduct astronomical research. ",
        "While most researchers are familiar with ChatGPT or Claude through their web interfaces, ",
        "the real power lies in programmatic access through APIs (Application Programming Interfaces). ",
        "This allows you to:\n",
        "\n",
        "- **Automate repetitive tasks**: Process hundreds of papers, generate summaries, or extract data\n",
        "- **Integrate AI into your workflow**: Connect LLMs with your existing analysis pipelines\n",
        "- **Build custom research tools**: Create specialized assistants for your specific needs\n",
        "- **Scale your research**: Process large datasets with consistent, reproducible AI assistance\n",
        "\n",
        "This tutorial series consists of two parts:\n",
        "1. **Part 1 (This lecture)**: Fundamentals of LLM APIs - making calls, managing conversations, prompting strategies\n",
        "2. **Part 2 (Next lecture)**: Advanced capabilities - function tools and Retrieval Augmented Generation (RAG)\n",
        "\n",
        "Together, these lectures will equip you to build AI-powered research agents tailored to astronomy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Programmatic Access Matters\n",
        "\n",
        "You might reasonably ask: \"If LLMs can write code for me through ChatGPT, why do I need to learn ",
        "to code with them?\" It's a fair question that deserves a thoughtful answer.\n",
        "\n",
        "**The Web Interface is Training Wheels**\n",
        "\n",
        "Using ChatGPT or Claude through a browser is like using a calculator\u2014it's great for one-off questions, ",
        "but imagine if you needed to do the same calculation 1,000 times. You wouldn't manually type each one. ",
        "Similarly, when you need to:\n",
        "- Analyze abstracts from 500 papers\n",
        "- Generate consistent summaries of observation logs\n",
        "- Create custom research tools that combine AI with your data\n",
        "\n",
        "...you need programmatic access.\n",
        "\n",
        "**Real Research Scenarios**\n",
        "\n",
        "Consider these common astronomy research tasks:\n",
        "- Extracting key findings from every paper on exoplanet atmospheres published this year\n",
        "- Generating natural language summaries of your spectroscopic observations\n",
        "- Building a custom chatbot that answers questions about your telescope's documentation\n",
        "- Creating an AI assistant that can both query astronomical databases AND reason about the results\n",
        "\n",
        "None of these are possible through a web interface alone. You need code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Chat to Code: The Fundamental Shift\n",
        "\n",
        "Up until now, your interaction with LLMs has probably been conversational and manual. ",
        "You type a question, get an answer, perhaps follow up. Each interaction is separate.\n",
        "\n",
        "**Programmatic access changes everything:**\n",
        "\n",
        "```python\n",
        "# Instead of manually asking about 100 papers...\n",
        "for paper in my_reading_list:\n",
        "    summary = ask_claude(f\"Summarize this paper: {paper.abstract}\")\n",
        "    key_findings = ask_claude(f\"What are the key findings?\")\n",
        "    save_to_database(paper.title, summary, key_findings)\n",
        "```\n",
        "\n",
        "This is the power we're unlocking today: **LLMs as programmable research tools**.\n",
        "\n",
        "By the end of this lecture, you'll understand how to integrate AI capabilities into your research workflow, ",
        "making them as accessible as any other Python library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding APIs\n",
        "\n",
        "Let's demystify this term that gets thrown around constantly in programming. API stands for Application Programming Interface, but that definition helps nobody. Here's a better way to think about it: an API is a structured way for programs to talk to each other. When your Python code needs to communicate with an LLM running on a server somewhere, the API defines exactly how that conversation happens.\n",
        "\n",
        "Think of it like ordering at a restaurant. You (your Python code) don't walk into the kitchen and start cooking. Instead, you look at a menu (the API documentation), place an order with specific requirements (the API request), and receive your meal (the API response). The waiter (the API) handles all the communication between you and the kitchen (the LLM). You don't need to know how the kitchen works, what equipment they use, or who's cooking\u2014you just need to know how to order.\n",
        "\n",
        "In concrete terms, when you want an LLM to analyze text, you send a specially formatted message over the internet to the LLM's servers. The message contains your prompt, along with parameters like which model to use and how long the response should be. The server processes your request and sends back a response containing the LLM's output. All of this happens in seconds, and it's all just Python dictionaries and strings\u2014data structures you already understand.\n",
        "\n",
        "The beauty of APIs is standardization. Whether you're using OpenAI's GPT, Anthropic's Claude, or Google's Gemini, the basic pattern remains the same: format your request, send it to the server, receive the response, extract the data. Once you learn this pattern with Claude today, you can apply it to any LLM service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Request-Response Dance\n",
        "\n",
        "Every API interaction follows a simple rhythm: request, response. You ask, the LLM answers. But unlike human conversation where ambiguity is common, APIs demand precision. You need to specify exactly what you want, in exactly the right format, or the server will politely refuse to help.\n",
        "\n",
        "Let's break down what actually happens when your Python code talks to an LLM:\n",
        "\n",
        "**Your Request Contains:**\n",
        "- **The Prompt**: Your actual question or task, written as a string\n",
        "- **The Model**: Which LLM version to use (e.g., claude-sonnet-4-20250514, claude-opus-4-1-20250805)\n",
        "- **Parameters**: Settings that control the response (like max_tokens for length, temperature for creativity)\n",
        "- **Your API Key**: A special password that identifies your account and tracks usage\n",
        "\n",
        "**The Response Delivers:**\n",
        "- **The Content**: The LLM's actual answer to your prompt\n",
        "- **Usage Information**: How many tokens were used (affects cost)\n",
        "- **Status Information**: Whether the request succeeded or failed\n",
        "- **Metadata**: Additional information like which model version actually processed your request\n",
        "\n",
        "This structure should feel familiar\u2014it's just like the functions you wrote in Lecture 5. You provide inputs (arguments), the function processes them, and you get outputs (return values). The only difference is that the processing happens on a remote server instead of your laptop.\n",
        "\n",
        "Understanding this request-response pattern is crucial because it's universal across all web APIs, not just LLMs. Whether you're fetching astronomical data from NASA's databases, weather data, or generating text with Claude, the pattern is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Your Connection\n",
        "\n",
        "### Getting Your API Key\n",
        "\n",
        "Before we write any code, you need to obtain an API key from an LLM provider. We'll focus on Anthropic's Claude for this lecture because of its strong performance on technical and scientific tasks, but the concepts apply to any LLM API.\n",
        "\n",
        "**Setting up Anthropic (Claude):**\n",
        "\n",
        "1. **Create an Account**\n",
        "   - Go to console.anthropic.com\n",
        "   - Click \"Sign up\" and create an account\n",
        "   - Verify your email address\n",
        "\n",
        "2. **Add Credits to Your Account**\n",
        "   - Once logged in, click on \"API Keys\" \u2192 \"Billing\" \u2192 \"Buy Credits\"\n",
        "   - Add at least $5 to get started (this will last for thousands of API calls)\n",
        "   - Enter your payment information\n",
        "   - Note: Without credits, your API calls will fail with an error about insufficient funds\n",
        "\n",
        "3. **Generate Your API Key**\n",
        "   - Navigate to \"API Keys\" \u2192 \"API Keys\" in your dashboard\n",
        "   - Click \"Create Key\"\n",
        "   - Give it a descriptive name like \"Astro1221 Project\"\n",
        "   - **Critical**: Copy the key immediately! It looks like this:\n",
        "     ```\n",
        "     sk-ant-api03-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "     ```\n",
        "   - You won't be able to see this key again after leaving this page\n",
        "   - Paste it somewhere temporary (like a text file on your desktop) - we'll move it to secure storage next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Tokens and Costs\n",
        " \n",
        "LLMs don't think in words\u2014they think in \"tokens,\" which are more like syllables than complete words. Here are some examples:\n",
        "- \"astronomy\" \u2192 [\"astro\", \"nomy\"] (2 tokens)\n",
        "- \"star\" \u2192 [\"star\"] (1 token)  \n",
        "- \"Hello, world!\" \u2192 [\"Hello\", \",\", \" world\", \"!\"] (4 tokens)\n",
        " \n",
        "**Rule of thumb:** 1,000 tokens \u2248 750 words\n",
        " \n",
        "When you send a prompt to Claude, you pay for input tokens. When Claude responds, you pay for output tokens. \n",
        "\n",
        "**The Model Tier Strategy:**\n",
        "\n",
        "Every LLM company offers multiple models at different price points\u2014think of it like choosing between a compact car and a luxury SUV. Both get you where you're going, but one is more economical while the other handles challenging terrain better.\n",
        "\n",
        "Anthropic offers two main Claude models:\n",
        "- **Claude Opus 4.1**: $15/$75 per million input/output tokens - the larger, more powerful model\u2014excellent for complex reasoning, multi-step problems, and tasks requiring deep understanding\n",
        "- **Claude Sonnet 4**: $3/$15 per million input/output tokens - smaller and faster, delivering 90-95% of Opus's quality at 20% of the cost\n",
        "\n",
        "For our class, we'll primarily use Claude Sonnet 4 because for simple tasks like extracting data from observation logs, it performs nearly as well as Opus. Let's break down the actual costs:\n",
        "\n",
        "**Example calculation for analyzing one observation log:**\n",
        "- Your prompt: ~100 tokens (asking to extract data)\n",
        "- Claude's response: ~150 tokens (JSON with extracted fields)\n",
        "- Total: 250 tokens\n",
        "\n",
        "With **Sonnet 4** pricing ($3/$15 per million tokens):\n",
        "- Input cost: 100 tokens \u00d7 ($3/1,000,000) = $0.0003\n",
        "- Output cost: 150 tokens \u00d7 ($15/1,000,000) = $0.00225\n",
        "- **Total per log: ~$0.0025**\n",
        "\n",
        "With **Opus 4.1** pricing ($15/$75 per million tokens):\n",
        "- Input cost: 100 tokens \u00d7 ($15/1,000,000) = $0.0015\n",
        "- Output cost: 150 tokens \u00d7 ($75/1,000,000) = $0.01125\n",
        "- **Total per log: ~$0.013**\n",
        "\n",
        "That's over 5x more expensive! With your $5 credit:\n",
        "- **Sonnet 4**: $5 \u00f7 $0.0025 = 2,000 observation logs\n",
        "- **Opus 4.1**: $5 \u00f7 $0.013 = 385 observation logs\n",
        "\n",
        "Since both models will extract galaxy names, dates, and observation parameters equally well, why pay more? Save Opus for when you need complex multi-step reasoning about the physical implications of your data. For straightforward extraction tasks, Sonnet is your workhorse.\n",
        "\n",
        "The art is matching the model to your task. Simple extraction? Use the cheaper model. Complex multi-step reasoning about the physical implications of your data? Consider the premium model. It's like choosing instruments for observations\u2014you don't need Hubble to observe the Moon, but you do need it for distant galaxies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keeping Your API Key Safe\n",
        "\n",
        "An API key is like your university ID card for the digital world. It proves to the LLM service that you're an authorized user and tracks your usage. Here's what an API key looks like (this one is fake):\n",
        "```\n",
        "sk-proj-7x9YZ2aBcDeFgHiJkLmNoPqRsTuVwXyZ123456789\n",
        "```\n",
        "\n",
        "The cardinal rule of API keys: **treat them like passwords**. Never, ever put them directly in your code. Why? Because the moment you share your notebook with a classmate, post it on GitHub for your portfolio, or even just show your screen during office hours, anyone who sees that key can use it to rack up charges on your account.\n",
        "\n",
        "Instead, we'll use an environment file\u2014a separate file that stores sensitive information and stays on your computer. Think of it as a safe in your coding workspace. Your code knows the combination to open the safe, but the safe itself never leaves your machine.\n",
        "\n",
        "### Creating your .env file\n",
        "\n",
        "Let's create a file called `.env` (yes, the filename starts with a dot\u2014this is a Unix convention that marks it as a hidden configuration file):\n",
        "\n",
        "1. In Cursor: File \u2192 New Text File\n",
        "2. Type your API key like this:\n",
        "   ```\n",
        "   ANTHROPIC_API_KEY=sk-ant-api03-your-actual-key-here\n",
        "   ```\n",
        "3. File \u2192 Save As \u2192 name it `.env`\n",
        "4. Save in your project folder (same folder as your notebooks)\n",
        "\n",
        "The format is simple: `VARIABLE_NAME=value` with no spaces around the equals sign, no quotes (unless they're part of the key itself). This format is called \"dotenv\" and it's an industry standard used by thousands of applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Protecting Your Keys with .gitignore\n",
        "\n",
        "While we'll cover Git version control in detail in Lecture 9, there's one critical safety measure you should know about now: the `.gitignore` file. This is your first line of defense against accidentally sharing your API key when you upload your code to GitHub or share your project.\n",
        "\n",
        "**What is .gitignore?**\n",
        "\n",
        "A `.gitignore` file tells Git (the version control system) which files to never upload to the internet. Think of it as a \"do not pack\" list when you're moving\u2014certain things stay private no matter what.\n",
        "\n",
        "**Creating your .gitignore file:**\n",
        "\n",
        "Create a new file called `.gitignore` in your project folder with this content:\n",
        "\n",
        "```gitignore\n",
        "# Environment variables\n",
        ".env\n",
        ".env.local\n",
        ".env.*.local\n",
        "\n",
        "# API keys and secrets\n",
        "*_api_key.txt\n",
        "secrets/\n",
        "\n",
        "# Python\n",
        "__pycache__/\n",
        "*.pyc\n",
        ".ipynb_checkpoints/\n",
        "```\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "This tells Git: \"Never upload any file named `.env`, any Python cache files, or Jupyter checkpoint files.\" Even if you accidentally run `git add .` (which adds everything), your `.env` file with your API key stays safe on your local machine.\n",
        "\n",
        "Think of `.gitignore` as a bouncer at a club\u2014it checks every file trying to get uploaded and turns away anything on its list. Your API keys never make it past the door.\n",
        "\n",
        "Even though you're not using Git yet, getting in the habit of using `.env` files and `.gitignore` means you'll never accidentally expose your API keys when you start sharing code. It's like learning to look both ways before crossing the street\u2014the habit protects you even when you're not thinking about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Your Keys in Python\n",
        "\n",
        "Now that your API key is safely stored in `.env`, we need to tell Python how to read it. We'll install two packages:\n",
        "\n",
        "1. **`python-dotenv`**: This package knows how to read `.env` files and load environment variables into Python. Without it, Python has no idea that `.env` files are special\u2014it would just see them as text files.\n",
        "\n",
        "2. **`anthropic`**: This is Anthropic's official Python library that handles all the complex networking details of talking to Claude. It manages HTTPS connections, formats your messages correctly, handles retries, and converts responses into Python objects.\n",
        "\n",
        "Think of it this way: `python-dotenv` is the key to your safe, and `anthropic` is your communication channel to Claude. You need both to make the API work securely.\n",
        "\n",
        "Let's install both packages:\n",
        "\n",
        "```bash\n",
        "pip install python-dotenv anthropic\n",
        "```\n",
        "\n",
        "With the packages installed, here's how to load your API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get your API key\n",
        "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "# Verify it loaded (but don't print the actual key!)\n",
        "if api_key:\n",
        "    print(\"\u2713 API key loaded successfully\")\n",
        "    print(f\"Key starts with: {api_key[:15]}...\")  # Just show the beginning\n",
        "else:\n",
        "    print(\"\u2717 No API key found. Check your .env file!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `load_dotenv()` function searches for a `.env` file in your current directory and loads all the variables it finds into Python's environment. Then `os.getenv('ANTHROPIC_API_KEY')` retrieves your specific key. It's like opening a safe (load_dotenv) and then taking out the specific item you want (os.getenv).\n",
        "\n",
        "If you see \"No API key found,\" check these common issues:\n",
        "- Is your `.env` file in the same folder as your notebook?\n",
        "- Did you spell `ANTHROPIC_API_KEY` exactly the same in both places?\n",
        "- Did you save the `.env` file after adding your key?\n",
        "- Are there spaces around the equals sign in your `.env` file? (There shouldn't be)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your First API Call\n",
        "\n",
        "Now comes the exciting moment\u2014actually talking to Claude through code instead of a chat interface. Let's start by creating our connection to Claude:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import anthropic\n",
        "import os\n",
        "\n",
        "# Create a client object - think of this as your connection to Claude\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=os.getenv('ANTHROPIC_API_KEY')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `client` object is your gateway to Claude. It handles authentication, manages the network connection, and provides methods for sending messages. Think of it as establishing a phone line to Claude\u2014once connected, you can have as many conversations as you want.\n",
        "\n",
        "Now let's make our first API call. Watch how every concept from your first weeks of Python comes together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make your first API call!\n",
        "message = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",  # Which model to use\n",
        "    max_tokens=100,                       # Maximum response length\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What type of galaxy is M31?\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Display the response\n",
        "print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding What Just Happened\n",
        "\n",
        "If everything worked, you should see Claude's response describing M31 (the Andromeda Galaxy) as a spiral galaxy. Congratulations\u2014you just programmatically consulted an LLM!\n",
        "\n",
        "But look closer at what just happened. Every single concept from your first weeks is at work:\n",
        "\n",
        "- **Variables (Lecture 2):** You stored the response in `message`\n",
        "- **Dictionaries (Lecture 2):** The message you sent has keys \"role\" and \"content\"\n",
        "- **Lists (Lecture 2):** The `messages` parameter takes a list of dictionaries\n",
        "- **List indexing (Lecture 2):** That `[0]` gets the first element\n",
        "- **Dot notation (Lecture 5):** `client.messages.create()` and `message.content[0].text`\n",
        "\n",
        "This might seem like a lot of machinery for a simple question, but here's the power: you can now process hundreds of astronomy questions, save responses to files, and build entire data processing pipelines. The complexity is front-loaded\u2014once you have this working, scaling to thousands of requests is just a matter of adding loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accessing Response Metadata\n",
        "\n",
        "The response contains more than just text. It includes valuable metadata about your API call, including token usage which directly relates to cost. Let's explore what else is available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a call and examine the full response\n",
        "message = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=500,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is a neutron star?\"}]\n",
        ")\n",
        "\n",
        "# The actual text response\n",
        "print(\"Response text:\")\n",
        "print(message.content[0].text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Token usage information\n",
        "print(\"Token usage:\")\n",
        "print(f\"Input tokens: {message.usage.input_tokens}\")\n",
        "print(f\"Output tokens: {message.usage.output_tokens}\")\n",
        "print(f\"Total tokens: {message.usage.input_tokens + message.usage.output_tokens}\")\n",
        "\n",
        "# Calculate the cost (using Sonnet pricing)\n",
        "input_cost = (message.usage.input_tokens / 1_000_000) * 3  # $3 per million\n",
        "output_cost = (message.usage.output_tokens / 1_000_000) * 15  # $15 per million\n",
        "total_cost = input_cost + output_cost\n",
        "\n",
        "print(f\"\\nEstimated cost for this call: ${total_cost:.6f}\")\n",
        "print(f\"Calls remaining with $5: {int(5 / total_cost) if total_cost > 0 else 'many'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding token usage is crucial for managing costs. By tracking this metadata, you can:\n",
        "- Monitor your spending in real-time\n",
        "- Optimize prompts to use fewer tokens\n",
        "- Predict costs before processing large batches\n",
        "- Set alerts when approaching budget limits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Parameters\n",
        "\n",
        "### The Model Parameter\n",
        "\n",
        "The `model` parameter determines which version of Claude you're talking to. Each model has different capabilities, speeds, and costs. Let's understand the differences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the standard model - fast and economical\n",
        "response_sonnet = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",  # Sonnet\n",
        "    max_tokens=50,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is a pulsar?\"}]\n",
        ")\n",
        "\n",
        "print(\"Sonnet says:\")\n",
        "print(response_sonnet.content[0].text)\n",
        "print(f\"\\nTokens used: {response_sonnet.usage.output_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For complex reasoning tasks, you could switch to Opus (though we'll stick with Sonnet for cost efficiency):\n",
        "```python\n",
        "model=\"claude-opus-4-1-20250805\"  # More expensive, more capable\n",
        "```\n",
        "\n",
        "The key is knowing when to use which model:\n",
        "- **Sonnet**: Data extraction, simple Q&A, formatting, basic analysis\n",
        "- **Opus**: Complex reasoning, multi-step problems, nuanced understanding, creative tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The max_tokens Parameter\n",
        "\n",
        "This parameter sets a hard limit on response length. Understanding its behavior is crucial because it affects both cost and completeness. Remember, tokens are roughly 3/4 of a word, so 100 tokens \u2248 75 words.\n",
        "\n",
        "When Claude hits the token limit, it stops mid-sentence without warning. Let's see this in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Very short limit - watch it get cut off!\n",
        "short_response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=20,  # Only about 15 words\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Explain how stars form from molecular clouds\"}]\n",
        ")\n",
        "\n",
        "print(\"With max_tokens=20:\")\n",
        "print(short_response.content[0].text)\n",
        "print(\"\\n[Notice how it cuts off mid-sentence!]\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Appropriate limit for a complete explanation\n",
        "full_response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=200,  # About 150 words\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Explain how stars form from molecular clouds\"}]\n",
        ")\n",
        "\n",
        "print(\"With max_tokens=200:\")\n",
        "print(full_response.content[0].text)\n",
        "print(f\"\\n[Complete response using {full_response.usage.output_tokens} tokens]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Choosing the right max_tokens:**\n",
        "- **50 tokens**: Yes/no answers, single facts, classifications\n",
        "- **100-200 tokens**: Short explanations, definitions\n",
        "- **500 tokens**: Detailed explanations, multiple paragraphs\n",
        "- **1000+ tokens**: Comprehensive analyses, long-form content\n",
        "\n",
        "Pro tip: Set max_tokens slightly higher than you expect to need. It's a ceiling, not a target\u2014Claude will stop naturally when done, using only the tokens it needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Temperature Parameter\n",
        "\n",
        "Temperature controls the randomness in Claude's responses. Think of it like tuning the creativity knob from \"strictly factual\" to \"creatively varied.\" The parameter ranges from 0.0 to 1.0:\n",
        "\n",
        "- **0.0**: Most deterministic, same input \u2192 nearly identical output\n",
        "- **0.5**: Balanced, some variation while staying on topic\n",
        "- **1.0**: Maximum creativity, more varied and sometimes surprising\n",
        "\n",
        "Let's see how temperature affects responses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Low temperature - very consistent and factual\n",
        "for i in range(3):\n",
        "    factual = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=30,\n",
        "        temperature=0.0,  # Minimum randomness\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Distance to Proxima Centauri?\"}]\n",
        "    )\n",
        "    print(f\"Try {i+1}: {factual.content[0].text}\")\n",
        "\n",
        "print(\"\\nNotice how the responses are nearly identical!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Higher temperature - more creative variation\n",
        "for i in range(3):\n",
        "    creative = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=50,\n",
        "        temperature=0.8,  # More creative\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Describe a sunset on Mars poetically\"}]\n",
        "    )\n",
        "    print(f\"Try {i+1}: {creative.content[0].text}\\n\")\n",
        "\n",
        "print(\"Notice the variety in descriptions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For astronomical data extraction, you'll almost always want low temperature (0.0-0.3) because you need consistent, reproducible results. Save higher temperatures for creative tasks like generating varied descriptions or brainstorming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### System Prompts: Setting the Context\n",
        "\n",
        "So far, we've only used \"user\" messages. But there's another powerful way to guide Claude: the \"system\" parameter. This sets the overall context and behavior for Claude, like giving it a specific role or persistent instructions.\n",
        "\n",
        "System prompts are especially useful when you want Claude to maintain a consistent persona or follow specific rules throughout a conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without system prompt - general response\n",
        "response_general = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=150,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What causes stellar parallax?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"General response:\")\n",
        "print(response_general.content[0].text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With system prompt - specialized response\n",
        "# Note: system is a parameter of messages.create(), not part of messages list!\n",
        "response_expert = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=150,\n",
        "    system=\"You are an astronomy professor teaching undergraduates. Explain concepts clearly with examples, and always mention the practical applications in modern astronomy.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What causes stellar parallax?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"With system prompt (professor mode):\")\n",
        "print(response_expert.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "System prompts are powerful for:\n",
        "- **Consistent formatting**: \"Always return responses as JSON\"\n",
        "- **Domain expertise**: \"You are an expert in stellar spectroscopy\"\n",
        "- **Output constraints**: \"Keep all responses under 50 words\"\n",
        "- **Behavioral rules**: \"Never make up data; say 'unknown' if not certain\"\n",
        "\n",
        "The system prompt persists across all messages in a conversation, making it perfect for maintaining consistent behavior throughout your data processing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Conversations\n",
        "\n",
        "Unlike the chat interface where Claude seems to \"remember\" your conversation, the API is completely stateless\u2014each API call is independent and has no memory of previous calls. To maintain context, you must explicitly provide the entire conversation history with each request.\n",
        "\n",
        "Let's demonstrate this crucial difference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's show that Claude does NOT remember without context\n",
        "print(\"Example 1: Without maintaining context (two separate calls)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# First call\n",
        "response1 = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=100,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"My favorite galaxy is M31. Remember this.\"}]\n",
        ")\n",
        "print(\"Call 1 - User: My favorite galaxy is M31. Remember this.\")\n",
        "print(\"Call 1 - Claude:\", response1.content[0].text)\n",
        "\n",
        "# Second call - Claude has NO memory of the first call\n",
        "response2 = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=100,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is my favorite galaxy?\"}]\n",
        ")\n",
        "print(\"\\nCall 2 - User: What is my favorite galaxy?\")\n",
        "print(\"Call 2 - Claude:\", response2.content[0].text)\n",
        "print(\"\\nNotice: Claude has no idea! Each API call is completely independent.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Context with Message History\n",
        "\n",
        "To create a conversation where Claude \"remembers,\" you must manually maintain the conversation history and send it with each request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nExample 2: Maintaining context by building conversation history\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Start with an empty conversation history\n",
        "conversation = []\n",
        "\n",
        "# First question\n",
        "conversation.append({\"role\": \"user\", \"content\": \"What is a pulsar?\"})\n",
        "\n",
        "# Get Claude's response\n",
        "response1 = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=150,\n",
        "    messages=conversation  # Just the user question\n",
        ")\n",
        "\n",
        "# CRITICAL: Add Claude's response to the conversation history\n",
        "# Without this, Claude won't \"remember\" what it said\n",
        "conversation.append({\n",
        "    \"role\": \"assistant\", \n",
        "    \"content\": response1.content[0].text\n",
        "})\n",
        "\n",
        "print(\"Q: What is a pulsar?\")\n",
        "print(\"A:\", response1.content[0].text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now ask a follow-up question\n",
        "conversation.append({\n",
        "    \"role\": \"user\", \n",
        "    \"content\": \"How fast do they typically spin?\"\n",
        "})\n",
        "\n",
        "# Claude now has the full context because we're sending the entire conversation\n",
        "response2 = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=150,\n",
        "    messages=conversation  # Now includes all previous messages\n",
        ")\n",
        "\n",
        "# Add this response too\n",
        "conversation.append({\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": response2.content[0].text\n",
        "})\n",
        "\n",
        "print(\"Q: How fast do they typically spin?\")\n",
        "print(\"A:\", response2.content[0].text)\n",
        "print(\"\\nNotice: Claude knows 'they' refers to pulsars because we sent the history!\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see what's actually being sent\n",
        "print(\"What we're actually sending to Claude:\")\n",
        "print(\"=\"*50)\n",
        "for i, msg in enumerate(conversation):\n",
        "    print(f\"Message {i+1} - {msg['role']}:\")\n",
        "    print(f\"  {msg['content'][:100]}...\" if len(msg['content']) > 100 else f\"  {msg['content']}\")\n",
        "    print()\n",
        "\n",
        "print(f\"Total messages in conversation: {len(conversation)}\")\n",
        "print(f\"Estimated tokens in context: ~{sum(len(m['content'])/4 for m in conversation):.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Key Insight\n",
        "\n",
        "Every time you make an API call, you're sending the **entire conversation history** to Claude. This is why:\n",
        "1. **Costs increase** with longer conversations (you're paying for all previous messages as input tokens)\n",
        "2. **You have complete control** over what Claude \"remembers\" (you can edit or remove messages)\n",
        "3. **Context windows matter** (eventually you hit the token limit and need to manage the history)\n",
        "\n",
        "This manual conversation management might seem tedious, but it's actually powerful\u2014you control exactly what context Claude sees!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Managing Long Conversations with Summarization\n",
        "\n",
        "As conversations grow, you face two challenges: rising costs (every old message counts as input tokens) and context limits (models have maximum token windows). The solution? Summarization.\n",
        "\n",
        "When your conversation history gets too long, you can ask Claude to summarize it, then start fresh with just the summary. This is like taking notes during a long meeting\u2014you keep the key points without all the back-and-forth.\n",
        "\n",
        "**Understanding the Problem:**\n",
        "\n",
        "Let's say you've been discussing different types of stellar objects with Claude. After 10 exchanges, your conversation history might be 2000 tokens. That means:\n",
        "- Every new question costs you those 2000 tokens as input\n",
        "- You're paying to send the same old messages over and over\n",
        "- Eventually you'll hit the context limit (typically 1M tokens for modern models)\n",
        " \n",
        "### What is a context limit? \n",
        "\n",
        "The context limit is the maximum number of tokens (input + output) that a model can process in a single request. Think of it as the model's \"working memory\"\u2014it can only keep track of so much text at once. When you exceed this limit, the API will reject your request or automatically truncate older messages.\n",
        "\n",
        "**The Summarization Solution:**\n",
        "\n",
        "Here's how we'll implement conversation summarization step by step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's build a conversation that's getting long\n",
        "print(\"Building a long conversation...\")\n",
        "\n",
        "long_conversation = []\n",
        "\n",
        "# Topic 1: Red giants\n",
        "long_conversation.append({\"role\": \"user\", \"content\": \"Tell me about red giant stars\"})\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=150,\n",
        "    messages=long_conversation\n",
        ")\n",
        "long_conversation.append({\"role\": \"assistant\", \"content\": response.content[0].text})\n",
        "\n",
        "tokens_so_far = sum(len(msg['content'])/4 for msg in long_conversation)\n",
        "print(f\"Turn 1: Asked about red giant stars ({tokens_so_far:.0f} tokens total)\")\n",
        "\n",
        "# Topic 2: White dwarfs\n",
        "long_conversation.append({\"role\": \"user\", \"content\": \"What about white dwarfs?\"})\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=150,\n",
        "    messages=long_conversation\n",
        ")\n",
        "long_conversation.append({\"role\": \"assistant\", \"content\": response.content[0].text})\n",
        "\n",
        "tokens_so_far = sum(len(msg['content'])/4 for msg in long_conversation)\n",
        "print(f\"Turn 2: Asked about white dwarfs ({tokens_so_far:.0f} tokens total)\")\n",
        "\n",
        "# Topic 3: Neutron stars\n",
        "long_conversation.append({\"role\": \"user\", \"content\": \"How do neutron stars form?\"})\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=150,\n",
        "    messages=long_conversation\n",
        ")\n",
        "long_conversation.append({\"role\": \"assistant\", \"content\": response.content[0].text})\n",
        "\n",
        "tokens_so_far = sum(len(msg['content'])/4 for msg in long_conversation)\n",
        "print(f\"Turn 3: Asked about neutron stars ({tokens_so_far:.0f} tokens total)\")\n",
        "\n",
        "print(f\"\\nConversation is getting long ({tokens_so_far:.0f} tokens). Time to summarize!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's create a summary of this conversation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's summarize the conversation\n",
        "print(\"Creating summary...\\n\")\n",
        "\n",
        "# Add a request for summarization to the conversation\n",
        "summary_request = long_conversation + [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Please provide a comprehensive summary of our entire conversation about stellar objects. Include the key facts we discussed about each type.\"\n",
        "}]\n",
        "\n",
        "# Get the summary\n",
        "summary_response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=300,\n",
        "    temperature=0.0,  # We want consistent summaries\n",
        "    messages=summary_request\n",
        ")\n",
        "\n",
        "summary_text = summary_response.content[0].text\n",
        "\n",
        "print(\"Summary of our conversation:\")\n",
        "print(\"=\"*50)\n",
        "print(summary_text)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Compare sizes\n",
        "original_tokens = sum(len(msg['content'])/4 for msg in long_conversation)\n",
        "summary_tokens = len(summary_text)/4\n",
        "\n",
        "print(f\"\\nOriginal conversation: {len(long_conversation)} messages, ~{original_tokens:.0f} tokens\")\n",
        "print(f\"Summary: 1 message, ~{summary_tokens:.0f} tokens\")\n",
        "print(f\"Token savings: {(1 - summary_tokens/original_tokens)*100:.0f}% reduction!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can start a new conversation with just the summary as context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start a new conversation with the summary\n",
        "compressed_conversation = [\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": f\"[Previous conversation summary]: {summary_text}\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What types of stellar remnants did we discuss?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Claude should still know what we talked about!\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=300,\n",
        "    messages=compressed_conversation\n",
        ")\n",
        "\n",
        "print(\"Starting fresh conversation with summary as context...\\n\")\n",
        "print(\"Question: What types of stellar remnants did we discuss?\\n\")\n",
        "print(\"Claude's response:\")\n",
        "print(response.content[0].text)\n",
        "print(\"\\nClaude successfully remembered the conversation through the summary!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**When to use summarization:**\n",
        "- After 10-15 conversation turns\n",
        "- When approaching 1000+ tokens in history  \n",
        "- Before context window limits (typically 200k tokens for Claude)\n",
        "- When switching between major topics\n",
        "\n",
        "**Pro tips for effective summarization:**\n",
        "1. Keep important numerical data and specific findings in the summary\n",
        "2. Consider keeping the last 2-3 messages unsummarized for immediate context\n",
        "3. You can create multiple summary levels (detailed \u2192 brief \u2192 key points only)\n",
        "4. Save full conversations before summarizing if you need complete records\n",
        "\n",
        "**Cost benefit example:**\n",
        "- Without summarization: 20-turn conversation = ~4000 input tokens per new message\n",
        "- With summarization: Same conversation = ~500 token summary\n",
        "- Savings: 87.5% reduction in input token costs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompting Strategies\n",
        "\n",
        "### Chain-of-Thought Prompting\n",
        "\n",
        "One of the most powerful techniques for improving LLM accuracy is \"chain-of-thought\" prompting\u2014explicitly asking the model to show its reasoning step-by-step. This is especially valuable for scientific calculations and multi-step problems.\n",
        "\n",
        "Let's compare responses with and without chain-of-thought:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without chain-of-thought - direct answer\n",
        "direct_response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=100,\n",
        "    temperature=0.0,\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"A star has a parallax of 0.05 arcseconds. What is its distance in parsecs and light-years?\"\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(\"Direct answer:\")\n",
        "print(direct_response.content[0].text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With chain-of-thought - step-by-step reasoning\n",
        "cot_response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=300,\n",
        "    temperature=0.0,\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"A star has a parallax of 0.05 arcseconds. What is its distance in parsecs and light-years?\n",
        "\n",
        "Please solve this step-by-step:\n",
        "1. State the parallax formula\n",
        "2. Show the calculation for parsecs\n",
        "3. Convert to light-years\n",
        "4. State the final answer\"\"\"\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(\"Chain-of-thought answer:\")\n",
        "print(cot_response.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chain-of-thought prompting helps in several ways:\n",
        "1. **Accuracy**: Breaking problems into steps reduces errors\n",
        "2. **Verifiability**: You can check each step of the reasoning\n",
        "3. **Learning**: The model's explanation helps you understand the solution\n",
        "4. **Debugging**: If wrong, you can identify exactly where the error occurred\n",
        "\n",
        "This technique is particularly powerful for:\n",
        "- Multi-step calculations\n",
        "- Complex logical reasoning\n",
        "- Data extraction with validation\n",
        "- Scientific analysis requiring justification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-Shot Prompting\n",
        "\n",
        "Another powerful technique is \"few-shot prompting\"\u2014providing examples of the input-output pattern you want. This is incredibly effective for consistent data extraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot (no examples) - might be inconsistent\n",
        "zero_shot = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=100,\n",
        "    temperature=0.0,\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Extract the observation data from: \n",
        "        'Observed NGC 1234 at 21:30 with 2m telescope, seeing 1.1 arcsec'\"\"\"\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(\"Zero-shot extraction:\")\n",
        "print(zero_shot.content[0].text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot (with examples) - consistent format\n",
        "few_shot = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=100,\n",
        "    temperature=0.0,\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Extract observation data to JSON. Examples:\n",
        "\n",
        "Input: 'Observed M31 at 22:00 with 1m telescope, seeing 0.9 arcsec'\n",
        "Output: {\"target\": \"M31\", \"time\": \"22:00\", \"telescope\": \"1m\", \"seeing\": 0.9}\n",
        "\n",
        "Input: 'Started on Jupiter at 23:15 using 3m scope, seeing 1.5 arcsec'\n",
        "Output: {\"target\": \"Jupiter\", \"time\": \"23:15\", \"telescope\": \"3m\", \"seeing\": 1.5}\n",
        "\n",
        "Now extract from: 'Observed NGC 1234 at 21:30 with 2m telescope, seeing 1.1 arcsec'\n",
        "Output:\"\"\"\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(\"Few-shot extraction:\")\n",
        "print(few_shot.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Few-shot prompting is perfect for:\n",
        "- **Consistent formatting**: The model mimics your example format exactly\n",
        "- **Edge case handling**: Examples can show how to handle special cases\n",
        "- **Complex patterns**: Examples are clearer than lengthy descriptions\n",
        "- **Quality control**: Examples set the quality bar for responses\n",
        "\n",
        "Rule of thumb: 2-3 good examples are usually better than a paragraph of instructions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making It Practical\n",
        "\n",
        "### Creating Helper Functions\n",
        "\n",
        "Now let's combine everything we've learned into practical, reusable functions for your research:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_claude(prompt, max_tokens=100, temperature=0.3, system=None):\n",
        "    \"\"\"Send a question to Claude and get a response.\"\"\"\n",
        "    # Build the basic parameters\n",
        "    params = {\n",
        "        \"model\": \"claude-sonnet-4-20250514\",\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "    \n",
        "    # Add system prompt if provided\n",
        "    if system:\n",
        "        params[\"system\"] = system\n",
        "    \n",
        "    message = client.messages.create(**params)\n",
        "    return message.content[0].text\n",
        "\n",
        "# Test it\n",
        "response = ask_claude(\n",
        "    \"What type of star is Betelgeuse?\",\n",
        "    system=\"You are an astronomy tutor. Keep answers concise.\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Rate Limits\n",
        "\n",
        "When you're processing hundreds of observation logs, you'll encounter **rate limits**\u2014restrictions on how many API calls you can make per minute. Think of it like a speed limit on a highway: the API provider wants to ensure their servers aren't overwhelmed and that all users get fair access.\n",
        "\n",
        "### What are rate limits?\n",
        "\n",
        "Rate limits typically come in two forms:\n",
        "1. **Requests per minute (RPM)**: How many API calls you can make (e.g., 50 requests/minute)\n",
        "2. **Tokens per minute (TPM)**: How many tokens you can process (e.g., 40,000 tokens/minute)\n",
        "\n",
        "When you hit a rate limit, the API returns a `RateLimitError` instead of a response. It's like getting a \"please wait\" message at a busy restaurant.\n",
        "\n",
        "**How to handle rate limits gracefully:**\n",
        "\n",
        "The simplest solution is to add pauses between requests using `time.sleep()`. This gives the server time to breathe between your requests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def process_with_rate_limit(items, delay=1.0):\n",
        "    \"\"\"Process items with a delay between each to avoid rate limits.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, item in enumerate(items):\n",
        "        # Process the item\n",
        "        result = ask_claude(f\"What type of star is {item}?\", max_tokens=20)\n",
        "        results.append(result)\n",
        "        \n",
        "        print(f\"Request {i+1}/{len(items)}: {result}\")\n",
        "        \n",
        "        # Sleep to avoid rate limits (except after the last item)\n",
        "        if i < len(items) - 1:\n",
        "            time.sleep(delay)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example: Process multiple stars with rate limiting\n",
        "stars = [\"Betelgeuse\", \"Rigel\", \"Sun\", \"Proxima Centauri\", \"Sirius B\"]\n",
        "\n",
        "print(\"Processing 5 requests with rate limiting...\")\n",
        "start_time = time.time()\n",
        "\n",
        "results = process_with_rate_limit(stars, delay=1.0)  # 1 second between requests\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Average time per request: {(end_time - start_time)/len(stars):.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced rate limit handling with exponential backoff\n",
        "\n",
        "Sometimes you don't know the exact rate limit, or it varies based on server load. A smarter approach uses exponential backoff\u2014starting with short delays and increasing them if you still hit limits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_with_backoff(prompt, max_retries=5):\n",
        "    \"\"\"Call API with exponential backoff for rate limits.\"\"\"\n",
        "    \n",
        "    delay = 0.5  # Start with 0.5 second delay\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Try the API call\n",
        "            response = ask_claude(prompt, max_tokens=30)\n",
        "            return response  # Success! Return the result\n",
        "            \n",
        "        except anthropic.RateLimitError as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                print(f\"Failed after {max_retries} attempts\")\n",
        "                raise e\n",
        "            \n",
        "            # Exponential backoff: double the delay each time\n",
        "            print(f\"Rate limited. Waiting {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # 0.5 \u2192 1 \u2192 2 \u2192 4 \u2192 8 seconds\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "            raise e\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "response = call_with_backoff(\"What types of stars are in the Orion Nebula?\")\n",
        "print(f\"Successfully processed: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rate limits might seem annoying, but they ensure the service stays reliable for everyone. With proper handling, they're just a minor speed bump in your data processing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding Error Handling\n",
        "\n",
        "Real-world API calls can fail for various reasons. Let's build robust error handling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def ask_claude_safely(prompt, max_tokens=100, temperature=0.3, system=None, max_retries=3):\n",
        "    \"\"\"Ask Claude with automatic retry on failure.\"\"\"\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return ask_claude(prompt, max_tokens, temperature, system)\n",
        "            \n",
        "        except anthropic.RateLimitError:\n",
        "            wait_time = 2 ** attempt  # Exponential backoff: 1, 2, 4 seconds\n",
        "            print(f\"Rate limited. Waiting {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "            \n",
        "        except anthropic.APIConnectionError:\n",
        "            print(f\"Connection error on attempt {attempt + 1}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(1)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {e}\")\n",
        "            return None\n",
        "    \n",
        "    print(f\"Failed after {max_retries} attempts\")\n",
        "    return None\n",
        "\n",
        "# Test error handling\n",
        "response = ask_claude_safely(\"What causes a supernova?\")\n",
        "if response:\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structured Outputs \n",
        "\n",
        "### From Text to Data\n",
        "\n",
        "So far, Claude returns plain text. But for those 200 observation logs, you want structured data you can analyze. Let's transform unstructured text into Python dictionaries and lists.\n",
        "\n",
        "Imagine you have this observation log:\n",
        "```\n",
        "\"Started observing at 22:15 UTC. Target: NGC 4526, spiral galaxy. \n",
        "Conditions were excellent with seeing at 0.8 arcsec. Used the \n",
        "2.4m telescope with R-band filter. Exposure time 300 seconds.\"\n",
        "```\n",
        "\n",
        "You want to extract:\n",
        "```python\n",
        "{\n",
        "    \"start_time\": \"22:15 UTC\",\n",
        "    \"target\": \"NGC 4526\",\n",
        "    \"object_type\": \"spiral galaxy\",\n",
        "    \"seeing\": 0.8,\n",
        "    \"telescope\": \"2.4m\",\n",
        "    \"filter\": \"R-band\",\n",
        "    \"exposure\": 300\n",
        "}\n",
        "```\n",
        "\n",
        "This structured format lets you analyze hundreds of logs programmatically!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instructing Claude to Return JSON\n",
        "\n",
        "The key to getting structured data is being explicit about the format you want. Clear instructions lead to consistent results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample observation log\n",
        "log = \"\"\"Started observing at 22:15 UTC. Target: NGC 4526, spiral galaxy. \n",
        "Conditions were excellent with seeing at 0.8 arcsec. Used the \n",
        "2.4m telescope with R-band filter. Exposure time 300 seconds.\"\"\"\n",
        "\n",
        "# Create a detailed prompt with clear instructions\n",
        "prompt = f\"\"\"Extract the following information from this observation log \n",
        "and return it as a JSON object:\n",
        "\n",
        "Observation log: {log}\n",
        "\n",
        "Return a JSON object with these fields (use null if not found):\n",
        "- start_time: when observation started\n",
        "- target: object name\n",
        "- object_type: type of astronomical object\n",
        "- seeing: seeing in arcseconds (number only)\n",
        "- telescope: telescope used\n",
        "- filter: filter used\n",
        "- exposure: exposure time in seconds (number only)\n",
        "\n",
        "Return ONLY the JSON object, no other text.\"\"\"\n",
        "\n",
        "# Get structured response\n",
        "json_text = ask_claude_safely(prompt, max_tokens=200, temperature=0.0)\n",
        "print(\"Raw JSON response:\")\n",
        "print(json_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting JSON to Python Dictionary\n",
        "\n",
        "The response is still just text! We need to parse it into a Python dictionary to work with it programmatically.\n",
        " \n",
        "Even with temperature=0.0, LLMs can produce varied output formats. Sometimes the JSON comes wrapped in markdown code blocks, sometimes with extra whitespace, or other formatting quirks. Robust string cleaning is essential - the techniques you learned in Lecture 2 will be very handy here!\n",
        " \n",
        "In the following example, we'll show how to handle JSON wrapped in markdown, but remember this is just one case. You'll need to adapt your parsing based on what the model actually returns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def parse_json_response(json_text):\n",
        "    \"\"\"Parse JSON response from Claude, handling common formatting issues.\"\"\"\n",
        "    try:\n",
        "        # Claude sometimes wraps JSON in markdown code blocks\n",
        "        if '```' in json_text:\n",
        "            # Extract content between ```\n",
        "            json_text = json_text.split('```')[1]\n",
        "            # Remove 'json' language identifier if present\n",
        "            if json_text.startswith('json'):\n",
        "                json_text = json_text[4:]\n",
        "        \n",
        "        # Parse the cleaned JSON\n",
        "        return json.loads(json_text.strip())\n",
        "        \n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Failed to parse JSON: {e}\")\n",
        "        print(f\"Raw text: {json_text}\")\n",
        "        return None\n",
        "\n",
        "# Parse the response\n",
        "data = parse_json_response(json_text)\n",
        "\n",
        "if data:\n",
        "    print(\"\\nExtracted data as Python dictionary:\")\n",
        "    for key, value in data.items():\n",
        "        print(f\"  {key}: {value} (type: {type(value).__name__})\")\n",
        "    \n",
        "    # Now we can work with it programmatically!\n",
        "    print(f\"\\nThe observation started at {data['start_time']}\")\n",
        "    print(f\"Seeing quality: {data['seeing']} arcsec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing Multiple Logs with Few-Shot Examples\n",
        "\n",
        "Let's combine structured extraction with few-shot prompting for consistent results across multiple logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_extraction_prompt(log):\n",
        "    \"\"\"Create a few-shot prompt for consistent extraction.\"\"\"\n",
        "    return f\"\"\"Extract observation data to JSON format.\n",
        "\n",
        "Examples:\n",
        "Log: \"At 20:30 UTC began observing M42 (Orion Nebula). Seeing 1.2 arcsec, using 1m telescope with H-alpha filter, 120 second exposure.\"\n",
        "JSON: {{\"time\": \"20:30 UTC\", \"target\": \"M42\", \"object_type\": \"Orion Nebula\", \"seeing\": 1.2, \"telescope\": \"1m\", \"filter\": \"H-alpha\", \"exposure\": 120}}\n",
        "\n",
        "Log: \"23:45 UTC: Target Vega (star) for calibration. Excellent seeing at 0.6 arcsec. 0.5m telescope, V-band filter, 30 seconds.\"\n",
        "JSON: {{\"time\": \"23:45 UTC\", \"target\": \"Vega\", \"object_type\": \"star\", \"seeing\": 0.6, \"telescope\": \"0.5m\", \"filter\": \"V-band\", \"exposure\": 30}}\n",
        "\n",
        "Now extract from this log:\n",
        "Log: \"{log}\"\n",
        "JSON:\"\"\"\n",
        "\n",
        "# Sample observation logs\n",
        "logs = [\n",
        "    \"Started at 19:45 UTC observing Betelgeuse (red supergiant). Seeing 0.95 arcsec, 2.5m telescope, R-band filter, 45 second exposure.\",\n",
        "    \"01:15 UTC: Switched to Saturn for planetary observations. Seeing deteriorated to 1.8 arcsec. Using 1.5m scope with methane filter, 90 seconds.\",\n",
        "    \"Began at 03:30 UTC on M13 (globular cluster). Good conditions, 0.7 arcsec seeing. 3m telescope, B-band filter, exposure 180 seconds.\"\n",
        "]\n",
        "\n",
        "# Process each log\n",
        "results = []\n",
        "\n",
        "for i, log in enumerate(logs, 1):\n",
        "    print(f\"Processing log {i}...\")\n",
        "    \n",
        "    # Get response with few-shot prompt\n",
        "    prompt = create_extraction_prompt(log)\n",
        "    response = ask_claude_safely(prompt, max_tokens=150, temperature=0.0)\n",
        "    \n",
        "    # Parse JSON\n",
        "    data = parse_json_response(response)\n",
        "    \n",
        "    if data:\n",
        "        results.append(data)\n",
        "        print(f\"  \u2713 Extracted: {data['target']} at {data['time']}\")\n",
        "    else:\n",
        "        print(f\"  \u2717 Failed to parse log {i}\")\n",
        "    \n",
        "    time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "# Analyze the extracted data\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Extracted Data Summary:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for obs in results:\n",
        "    print(f\"\\nTarget: {obs['target']}\")\n",
        "    print(f\"  Type: {obs['object_type']}\")\n",
        "    print(f\"  Time: {obs['time']}\")\n",
        "    print(f\"  Seeing: {obs['seeing']} arcsec\")\n",
        "    print(f\"  Setup: {obs['telescope']} telescope, {obs['filter']} filter\")\n",
        "    print(f\"  Exposure: {obs['exposure']} seconds\")\n",
        "\n",
        "# Calculate statistics\n",
        "if results:\n",
        "    seeing_values = [obs['seeing'] for obs in results]\n",
        "    avg_seeing = sum(seeing_values) / len(seeing_values)\n",
        "    print(f\"\\nAverage seeing across all observations: {avg_seeing:.2f} arcsec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vision Models - Beyond Text\n",
        "\n",
        "Modern AI models aren't limited to just text\u2014they can also analyze and understand images. This opens up exciting possibilities for astronomy, where visual data is fundamental to our observations and discoveries. In this section, we'll explore how to work with vision models using our astronomical images.\n",
        "\n",
        "### First, Let's Display Our Astronomical Image\n",
        "\n",
        "Before we dive into how to send images to AI models, let's first look at the image we'll be working with using matplotlib (as we learned in Lecture 6). We'll use `matplotlib.image.mpimg.imread()` to load our image file - this function reads image files (like JPEG, PNG, etc.) and converts them into NumPy arrays that Python can work with:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Load and display the M31 image\n",
        "img = mpimg.imread(\"data_m31.jpg\")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(img)\n",
        "plt.title('M31 - The Andromeda Galaxy', fontsize=16, fontweight='bold')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Image shape: {img.shape}\")\n",
        "print(f\"Image data type: {img.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Base64 Encoding\n",
        "\n",
        "Now that we can see our beautiful astronomical image, let's understand how to send it to AI models. Images are binary data\u2014they can't be sent as plain text in a JSON message. The solution is **base64 encoding**, which converts binary data (your image) into text characters that can be transmitted over the internet.\n",
        "\n",
        "Think of it like this: you want to send a photograph through a system that only handles text messages. Base64 encoding \"translates\" your image into a long string of letters and numbers that can be sent as text, then decoded back into an image on the other end.\n",
        "\n",
        "Here's how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "# Let's see what base64 encoding looks like\n",
        "def demonstrate_base64():\n",
        "    # Start with simple text\n",
        "    text = \"Hello, Astronomy!\"\n",
        "    \n",
        "    # Convert to bytes\n",
        "    text_bytes = text.encode('utf-8')\n",
        "    \n",
        "    # Encode to base64\n",
        "    encoded = base64.b64encode(text_bytes).decode('ascii')\n",
        "    \n",
        "    print(f\"Original text: {text}\")\n",
        "    print(f\"Base64 encoded: {encoded}\")\n",
        "    print(f\"Length increased from {len(text)} to {len(encoded)} characters\")\n",
        "    \n",
        "    # Decode back\n",
        "    decoded_bytes = base64.b64decode(encoded)\n",
        "    decoded_text = decoded_bytes.decode('utf-8')\n",
        "    print(f\"Decoded back: {decoded_text}\")\n",
        "\n",
        "demonstrate_base64()\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Now for an actual image file\n",
        "def encode_image_demo(image_path):\n",
        "    \"\"\"Show how image encoding works.\"\"\"\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        image_bytes = f.read()\n",
        "    \n",
        "    # Encode to base64\n",
        "    encoded = base64.b64encode(image_bytes).decode('ascii')\n",
        "    \n",
        "    print(f\"Image file: {image_path}\")\n",
        "    print(f\"Original size: {len(image_bytes):,} bytes\")\n",
        "    print(f\"Base64 size: {len(encoded):,} characters\")\n",
        "    print(f\"First 100 characters of encoded image: {encoded[:100]}...\")\n",
        "    print(f\"\\nThis string representation of the image can be sent through the API!\")\n",
        "    \n",
        "    return encoded\n",
        "\n",
        "# Encode your M31 image\n",
        "encoded_image = encode_image_demo(\"m31.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing Astronomical Images\n",
        "\n",
        "Now that we understand base64 encoding, let's build a function to analyze astronomical images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_astronomical_image(image_path, question):\n",
        "    \"\"\"Send an astronomical image to Claude for analysis.\"\"\"\n",
        "    \n",
        "    # Read the image file as binary data\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        image_data = f.read()\n",
        "    \n",
        "    # Convert to base64 - this is how we send binary data as text\n",
        "    image_base64 = base64.b64encode(image_data).decode('ascii')\n",
        "    \n",
        "    # Create message with both text and image\n",
        "    message = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=300,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": question},\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"source\": {\n",
        "                        \"type\": \"base64\",\n",
        "                        \"media_type\": \"image/jpeg\",  # Specify the image type\n",
        "                        \"data\": image_base64  # The encoded image data\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }]\n",
        "    )\n",
        "    \n",
        "    return message.content[0].text\n",
        "\n",
        "# Analyze the M31 galaxy image\n",
        "analysis = analyze_astronomical_image(\n",
        "    \"m31.jpg\",\n",
        "    \"\"\"Analyze this astronomical image:\n",
        "    1. What type of object is shown?\n",
        "    2. Describe its morphological features\n",
        "    3. What can you infer about its structure?\n",
        "    4. Are there any companion objects visible?\"\"\"\n",
        ")\n",
        "\n",
        "print(\"Image Analysis:\")\n",
        "print(analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combining Text and Image Analysis\n",
        "\n",
        "The real power comes from analyzing both observation logs and their resulting images together. This creates a complete picture of your observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_log_with_image(log_text, image_path):\n",
        "    \"\"\"Compare an observation log with the resulting image.\"\"\"\n",
        "    \n",
        "    prompt = f\"\"\"Compare this observation log with the astronomical image:\n",
        "    \n",
        "OBSERVATION LOG:\n",
        "{log_text}\n",
        "\n",
        "Please analyze:\n",
        "1. Does the image quality match the reported seeing conditions?\n",
        "2. Is the target object clearly identifiable?\n",
        "3. Are there any unexpected features or issues visible?\n",
        "4. Overall assessment of observation success.\n",
        "\n",
        "Provide your analysis in JSON format with fields:\n",
        "quality_match, target_visible, unexpected_features, success_rating (1-10), notes\"\"\"\n",
        "    \n",
        "    # Read and encode image\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        image_data = base64.b64encode(f.read()).decode('ascii')\n",
        "    \n",
        "    # Send both log and image\n",
        "    message = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=400,\n",
        "        temperature=0.0,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"source\": {\n",
        "                        \"type\": \"base64\",\n",
        "                        \"media_type\": \"image/jpeg\",\n",
        "                        \"data\": image_data\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }]\n",
        "    )\n",
        "    \n",
        "    return message.content[0].text\n",
        "\n",
        "# Example observation log\n",
        "observation_log = \"\"\"Observed M31 (Andromeda Galaxy) at 22:00 UTC. \n",
        "Conditions: Excellent, seeing 0.8 arcsec\n",
        "Equipment: 2.5m telescope, R-band filter\n",
        "Exposure: 120 seconds\n",
        "Notes: Clear skies, no moon interference\"\"\"\n",
        "\n",
        "# Compare log with image\n",
        "comparison = compare_log_with_image(observation_log, \"m31.jpg\")\n",
        "print(\"Log-Image Comparison:\")\n",
        "print(comparison)\n",
        "\n",
        "# Parse the JSON result\n",
        "comparison_data = parse_json_response(comparison)\n",
        "if comparison_data:\n",
        "    print(f\"\\nSuccess Rating: {comparison_data.get('success_rating')}/10\")\n",
        "    print(f\"Notes: {comparison_data.get('notes')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vision Model Capabilities and Limitations\n",
        "\n",
        "**What vision models CAN do well:**\n",
        "- Classify galaxy morphologies (spiral, elliptical, irregular)\n",
        "- Identify obvious features (spiral arms, dust lanes, star clusters)\n",
        "- Compare images qualitatively\n",
        "- Describe general image quality and artifacts\n",
        "\n",
        "**What they CANNOT do:**\n",
        "- Process raw FITS files (use astropy for this)\n",
        "- Perform precise photometry or astrometry\n",
        "- Make quantitative measurements\n",
        "- Detect faint features below visual threshold\n",
        "- Replace proper astronomical image analysis software\n",
        "\n",
        "Vision models are best used as a first-pass analysis tool or for processing large numbers of images where perfect accuracy isn't required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've just learned the fundamentals of working with Large Language Model APIs. ",
        "Let's recap what you've mastered:\n",
        "\n",
        "**Core Concepts:**\n",
        "- How APIs enable programmatic access to LLMs\n",
        "- Making basic API calls with proper authentication\n",
        "- Understanding tokens, costs, and rate limits\n",
        "\n",
        "**Key Parameters:**\n",
        "- `max_tokens`: Controlling response length\n",
        "- `temperature`: Balancing creativity vs. consistency\n",
        "- `system`: Setting context and behavior\n",
        "\n",
        "**Advanced Techniques:**\n",
        "- Building multi-turn conversations with message history\n",
        "- Managing long conversations through summarization\n",
        "- Crafting effective prompts with few-shot examples\n",
        "- Extracting structured data (JSON) from LLM responses\n",
        "- Working with vision models for image analysis\n",
        "- Implementing error handling and rate limit management\n",
        "\n",
        "**You're Now Ready For:**\n",
        "\n",
        "These skills form the foundation for everything we'll build in Part 2. You can now:\n",
        "- Automate repetitive research tasks\n",
        "- Build custom AI tools for your workflow\n",
        "- Process data at scale with consistent AI assistance\n",
        "\n",
        "In **Part 2: Function Tools and RAG**, we'll extend these capabilities dramatically by:\n",
        "- Teaching Claude to use external functions (perform calculations, query databases)\n",
        "- Implementing Retrieval Augmented Generation to work with your documents\n",
        "- Building complete research assistants that combine reasoning with real data\n",
        "\n",
        "**Practice Exercises:**\n",
        "\n",
        "Before moving to Part 2, try building:\n",
        "1. A script that summarizes astronomical paper abstracts from a CSV file\n",
        "2. A conversation analyzer for your observing logs\n",
        "3. An image classification tool for telescope data\n",
        "\n",
        "**Resources:**\n",
        "- Anthropic API Documentation: https://docs.anthropic.com\n",
        "- Claude Model Card: https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf\n",
        "- This tutorial's GitHub: https://github.com/tingyuansen/NASA_AI_ML_STIG\n",
        "\n",
        "See you in Part 2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}